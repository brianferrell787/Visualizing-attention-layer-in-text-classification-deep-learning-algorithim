# Visualizing-attention-layer-in-text-classification-deep-learning-algorithim
I have finished creating a prototype algorithm on a small dataset for work that can detect the gradients of a newly created classification of Community Engaged Research. This model has a custom attention layer added in and I have found that visualizing this layer can actually help see what words are important and what words are not. Although there may be some skepticism on whether or not attention layers can in fact explain a model(SEE https://arxiv.org/abs/1902.10186), there are other articles out there that use this technique. I have found it to be very helpful, and it aligns with what I saw in the data when creating this new way of classifying Community Engaged Research Protocols. Hope you enjoy and please critique!!!


# Acknowledgements:
Attention Viz was inspired by this StackOverflow post: 
https://stackoverflow.com/questions/53867351/how-to-visualize-attention-weights

As well as these research papers: 
https://arxiv.org/abs/1902.02181
https://arxiv.org/abs/1805.12307

Code for attention layer was used from:
https://www.analyticsvidhya.com/blog/2019/11/comprehensive-guide-attention-mechanism-deep-learning/


#**Attention Layer Viz Example**
![Alt text](examplesattentions.JPG?raw=true "Title")
